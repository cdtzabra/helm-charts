# Default values for cnpg-cluster.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Cluster Instance
instances: 3
#
postgresUID: 26 # UID of the postgres user
postgresGID: 26
startDelay: 300
stopDelay: 300
primaryUpdateStrategy: unsupervised # it can be automated (unsupervised - default) or manual (supervised)
primaryUpdateMethod: restart # it can be with a switchover (switchover) or in-place (restart - default)
#
# Minimum number of instances required in synchronous replication with the primary. Undefined or 0 allow writes to complete when no standby is available
minSyncReplicas: 0
# The target value for the synchronous replication quorum, that can be decreased if the number of ready standbys is lower than this. Undefined or 0 disable synchronous replication.
maxSyncReplicas: 0
logLevel: info
# priorityClassName: "" # optionnal

image:
  repository: ghcr.io/cloudnative-pg/postgresql
  tag: "16.0"
  pullPolicy: IfNotPresent

# imagePullSecret is only required if the images are located in a private registry
imagePullSecrets: []
# imagePullSecrets:
#    - name: private_registry_access


# Labels & Annotations
# https://cloudnative-pg.io/documentation/1.22/labels_annotations/

# only labels keys present in the cloudnative-pg controller's INHERITED_LABELS
# will be propagated from the cluster to the post-created pods
labels: {}
# labels:
#   component: postgres

# only annotation keys present in the cloudnative-pg controller's INHERITED_ANNOTATIONS
# will be propagated from the cluster to the post-created pods
annotations: {}
# annotations:
#   pre.hook.backup.velero.io/container: postgres
#   pre.hook.backup.velero.io/on-error: Fail
#   pre.hook.backup.velero.io/timeout: 90s

# postgresql
pg:
  parameters:
    # PostgreSQL configuration options (postgresql.conf)
    shared_buffers: 256MB
    pg_stat_statements.max: '10000'
    pg_stat_statements.track: all
    auto_explain.log_min_duration: '10s'
  pg_hba: []
  # pg_hba:
  #   - "host all all 10.244.0.0/16 md5"
  # the maximum number of seconds to wait when promoting an instance to primary. 
  promotionTimeout: '40000000'
  # Lists of shared preload libraries to add to the default ones
  shared_preload_libraries: []

# boostrap: initdb(Default) OR recover OR pg_basebackup => Mutual Exclusive

bootstrap_use_initdb: true
bootstrap_use_recovery: false
bootstrap_use_pg_basebackup: false
# all the three are true, initdb will be applied
bootstrap:
  initdb:
    # Bootstrap the cluster via initdb
    database: "" # default database to create. if unset app will be used
    owner: ""    # default database user. if unset app will be used
    owner_secret: "" # if unset secret will be generated by cnpg
    # the existing secret must have the keys: username and password
    dataChecksums: false
    # wal-segsize in MB
    encoding: "UTF8"
    walSegmentSize: "" #  WAL segment size,  must be a power of 2
    #
    # List of SQL queries to be executed as a superuser immediately after the cluster has been created - to be used with extreme care
    postInitSQL: []
    # postInitSQL:
    #   - create table numbers (i integer)
    #   - insert into numbers (select generate_series(1,10000))
    #
    # List of SQL queries to be executed as a superuser in the template1 after the cluster has been created - to be used with extreme care
    postInitTemplateSQL: []
    # postInitTemplateSQL:
    #   - create extension intarray
    #
    # List of SQL queries to be executed as a superuser in the application database right after is created - to be used with extreme care
    # postInitApplicationSQL: []
    # postInitApplicationSQL:
    #   - create table application_numbers (i integer)
    #
    # PostInitApplicationSQLRefs points references to ConfigMaps or Secrets which contain SQL files
    postInitApplicationSQLRefs: {}
    # postInitApplicationSQLRefs:
    # # you must create the secret or confimap first before using them here
    #   configMapRefs:
    #   - name: post-init-sql-configmap
    #     key: configmap.sql
    #   secretRefs:
    #   - name: post-init-sql-secret
    #     key: secret.sql
    #
    # Bootstraps the new cluster by importing data from an existing PostgreSQL instance using logical backup (pg_dump and pg_restore)
    # https://cloudnative-pg.io/documentation/current/database_import/
    import: {}
    # import:
    #   type: microservice
    #   databases:
    #     - skdb
    #   source:
    #     externalCluster: externalcluster
  # https://cloudnative-pg.io/documentation/1.22/cloudnative-pg.v1/#postgresql-cnpg-io-v1-BootstrapRecovery
  recovery:
    # Bootstrap the cluster from 
    # - from_backup: The backup object containing the physical base backup from which to initiate the recovery procedure. Mutually exclusive with source and volumeSnapshots.
    # - from_source_cluster: The external cluster whose backup we will restore. This is also used as the name of the folder under which the backup is stored, 
    #                so it must be set to the name of the source cluster. Mutually exclusive with backup
    # - from_volume: The static PVC data source(s) from which to initiate the recovery procedure. Currently supporting VolumeSnapshot and PersistentVolumeClaim resources 
    #                that map an existing PVC group compatible with CloudNativePG. Mutually exclusive with backup
    #
    from_backup: ""
    from_source_cluster: ""
    from_volume:
      snapshot: ""
      walStorage: "" # Optionnal, for the case the backed-up cluster was using a separate PVC to store the WAL files
    #
    #
    # By default, the recovery process applies all the available WAL files in the archive (full recovery).
    # However, you can also end the recovery as soon as a consistent state is reached or recover to a point-in-time (PITR)
    # https://cloudnative-pg.io/documentation/1.22/cloudnative-pg.v1/#postgresql-cnpg-io-v1-RecoveryTarget
    recoveryTarget: {}
    # recoveryTarget: 
    #   backupID: "xx"
    #   targetName: "yz"
    #   targetLSN: "zx"
    #
    # optional values related to the default/init database
    database: ""
    database_owner: ""
    database_owner_secret: ""
  #
  pg_basebackup:
  # Bootstrap the cluster taking a physical backup of another compatible PostgreSQL instance
  # REQUIRES externalClusters to be set. Variable is available below
    # The external of which we need to take a physical backup
    source_cluster: ""
    # optional values related to the default/init database
    database: ""
    database_owner: ""
    database_owner_secret: ""

# replica from an external cluster 
# REQUIRES externalClusters to be set replica
# https://cloudnative-pg.io/documentation/1.21/replica_cluster/
replica:
  enabled: false
  source: ""  # the external-cluster

# externalClusters
# ExternalCluster represents the connection parameters to an external cluster which is used in the other sections of the configuration
externalClusters: {}
# externalClusters:
# - name: cluster-example
#   connectionParameters:
#     host: cluster-example-rw.default.svc
#     user: streaming_replica
#     sslmode: verify-full
    # NOTE: if this cluster is created in a different namespace than the main cluster
    # remember to create the `-replication` and `-ca` secrets in the follower namespace
    # before creating the follower cluster
#   sslKey:
#     name: cluster-example-replication
#     key: tls.key
#   sslCert:
#     name: cluster-example-replication
#     key: tls.crt
#   sslRootCert:
#     name: cluster-example-ca
#     key: ca.crt 


# superuser
enableSuperuserAccess: true
superuser_existingsecret: "" # if unset secret will be generated by cnpg
# the existing secret must have the keys: username and password

# storage
storage:
  resizeInUseVolumes: true
  size: 10Gi
  storageClassName: "" # optional
  volumeMode: "" # optional
  pvcTemplate: {} # optional - customize the pvc template
  # pvcTemplate:
  #   accessModes:
  #     - ReadWriteOnce
  #   resources:
  #     requests:
  #       storage: 1Gi

# walStorage
# to store the wal in separate pv
walstorage_enabled: false
walstorage:
  resizeInUseVolumes: true
  size: 3Gi
  storageClassName: "" # optional
  volumeMode: "" # optional
  pvcTemplate: {} # optional - customize the pvc template
  # pvcTemplate:
  #   accessModes:
  #     - ReadWriteOnce
  #   resources:
  #     requests:
  #       storage: 1Gi

# backup
backup_enabled: false # enalbe/disable backup to s3
backup:
  target: "" # value MUST BE ""(empty) or prefer-standby: to have backups run preferably on the most updated standby
  retentionPolicy: "15d"
  s3_endpointURL: ""      # https://s3.example.com
  s3_destinationPath: ""  # s3://bucket_name/path/to/folder
  s3_credential:
    secretName: ""
    secret_accesKey: "ACCESS_KEY_ID"  # accesskey value in the secret resource
    secret_secretKey: "ACCESS_SECRET_KEY" # secretkey value in the secret resource
    # apiVersion: v1
    # kind: Secret
    # metadata:
    #   name: s3-backup-creds
    # data:
    #   ACCESS_KEY_ID: a2V5X2lk
    #   ACCESS_SECRET_KEY: c2VjcmV0X2tleQ==
    #
  # EndpointCA store the CA bundle of the barman endpoint.
  # Useful when using self-signed certificates to avoid errors with certificate issuer and barman-cloud-wal-archive
  endpointCA: {}
  # endpointCA:
  #   name: ""
  #   key: ""
  # wal
  wal:
    compression: gzip
    encryption: "" # Allowed options are empty string (use the bucket policy, default), AES256 and aws:kms
    maxParallel: 1 # Number of WAL files to be either archived in parallel or restored in parallel
    #                # If not specified, WAL files will be processed one at a time 1 is the minimum accepted value."
  # data
  data:
    compression: gzip
    encryption: ""
    jobs: 2 # The number of parallel jobs to be used to upload the backup
    immediateCheckpoint: false # default
    # Control whether the I/O workload for the backup initial checkpoint will be limited
    # according to the checkpoint_completion_target setting on the PostgreSQL server.
    # If set to true, an immediate checkpoint will be used, meaning PostgreSQL will complete the checkpoint as soon as possible.
  #
  # To perform volume snapshot during backup also
  volumeSnapshot:
    enabled: false
    className: ""     # optional
    walClassName: ""  # optional
    online: true 
    # Whether the default type of backup with volume snapshots is online/hot (true, default) or offline/cold (false)


############## Scheduled Backup Resource #######################
# If you have enabled the backup for your cluster above, you have the possibility to rely on it to create a separate scheduledbackup resource.
# This scheduledbackup resource will get S3 information from the backup settings. So It will not work if the backup is not enabled and configured in the cluster
#
# Scheduled backups are the recommended way to configure your backup strategy in CloudNativePG. They are managed by the ScheduledBackup resource.
# https://cloudnative-pg.io/documentation/1.21/backup/

scheduledbackup:
  enabled: false
  # perform an immediate backup as soon as the resource is created
  immediate: false
  schedule: "0 0 0 * * *"
  # Beware that this format accepts also the seconds field, and it is different from the crontab format in Unix/Linux systems.
  # it has a total of 6 fiels while kubernetes/linux cronjob has 5


# nodeMaintenanceWindow
# NodeMaintenanceWindow contains information that the operator will use while upgrading the underlying node.
# This option is only useful when the chosen storage prevents the Pods from being freely moved across nodes.
nodeMaintenanceWindow:
  inProgress: false
  reusePVC: false
  # Reuse the existing PVC (wait for the node to come up again) or not (recreate it elsewhere - when instances >1)

# monitoring
monitoring:
  enablePodMonitor: true

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# env
## Add custom env variables for pods
env: []
# env:
#   - name: "Author"
#     value: "SKONE"

# ServiceAccountTemplate contains the template needed to generate the service accounts
# https://cloudnative-pg.io/documentation/1.21/cloudnative-pg.v1/#postgresql-cnpg-io-v1-ServiceAccountTemplate
ServiceAccountTemplate:
  # to add or not the ServiceAccountTemplate in cluster manifest
  enabled: false
  labels: {}
    # author: "skone"
  annotations: {}
    # openshift.io/sa.scc.uid-range: "1001xxxx/10015xxx"

# affinity
affinity:
  enablePodAntiAffinity: false
  podAntiAffinityType: "preferred" # Allowed values are: "preferred" (default if empty) or "required"
  topologyKey: failure-domain.beta.kubernetes.io/zone
  nodeSelector: {}
  tolerations: []


# managed - Roles
# RoleConfiguration is the representation, in Kubernetes, of a PostgreSQL role with the additional field
# Ensure specifying whether to ensure the presence or absence of the role in the database
roles: []
# roles:
# - name: app
#   createdb: true
#   login: true
# - name: dante
#   ensure: present
#   comment: my database-side comment
#   login: true
#   superuser: false
#   createdb: true
#   createrole: false
#   inherit: false
#   replication: false
#   bypassrls: false
#   connectionLimit: 4
#   validUntil: "2053-04-12T15:04:05Z"
#   inRoles:
#     - pg_monitor
#     - pg_signal_backend
#   passwordSecret:
#     name: cluster-example-dante

# certificates
# https://cloudnative-pg.io/documentation/current/cloudnative-pg.v1/#postgresql-cnpg-io-v1-CertificatesConfiguration
certificates: {}
# certificates:
#   serverAltDNSNames:
#     - skone.test.com
#   clientCASecret: ""
#   replicationTLSSecret: ""
#   serverTLSSecret: ""
#   serverCASecret: ""